{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\nimport json\nimport os\nimport zipfile\n\n# 크롤링할 URL 패턴\nbase_url = \"https://n.news.naver.com/mnews/article/023/{:010d}\"\n\n# User-Agent 설정\nheaders = {\"User-Agent\": \"Mozilla/5.0\"}\n\n# 저장할 디렉터리 생성\noutput_dir = \"naver_news\"\nos.makedirs(output_dir, exist_ok=True)\n\n# 기사 번호 범위 설정\nfor article_num in range(3898397, 3898467):  # 범위 수정\n    url = base_url.format(article_num)\n    response = requests.get(url, headers=headers)\n    \n    if response.status_code == 200:\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        \n        # 제목 크롤링\n        title_tag = soup.select_one(\"h2#title_area\")\n        title = title_tag.text.strip() if title_tag else \"제목 없음\"\n        \n        # 본문 크롤링\n        article_tag = soup.select_one(\"article#dic_area\")\n        if article_tag:\n            for tag in article_tag.find_all([\"strong\", \"em\", \"span\"]):\n                tag.decompose()\n            content = article_tag.get_text(separator=\"\\n\").strip()\n        else:\n            content = \"본문 없음\"\n        \n        # 입력 시간 크롤링\n        date_tag = soup.select_one(\"span.media_end_head_info_datestamp_time\")\n        news_date = date_tag.text.strip() if date_tag else \"날짜 없음\"\n        \n        # 카테고리 크롤링\n        category_tag = soup.select_one(\"em.media_end_categorize_item\")\n        news_category = category_tag.text.strip() if category_tag else \"카테고리 없음\"\n        \n        # 기자 정보 크롤링\n        reporter_tag = soup.select_one(\"span.byline_s\")\n        news_reporter = reporter_tag.text.strip() if reporter_tag else \"기자 정보 없음\"\n        \n        # JSON 데이터 생성\n        news_data = {\n            \"sourceDataInfo\": {\n                \"newsTitle\": title,\n                \"newsContent\": content,\n                \"newsCompany\": \"조선일보\",\n                \"newsDate\": news_date,\n                \"newsCategory\": news_category,\n                \"newsReporter\": news_reporter,\n            }\n        }\n        \n        # 파일로 저장\n        file_path = os.path.join(output_dir, f\"JO_news_{article_num:010d}.json\")\n        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(news_data, f, ensure_ascii=False, indent=4)\n    \n    # 요청 실패 시 패스\n    else:\n        continue\n\n# ZIP 파일로 묶기\nzip_filename = \"naver_news.zip\"\nwith zipfile.ZipFile(zip_filename, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n    for root, _, files in os.walk(output_dir):\n        for file in files:\n            zipf.write(os.path.join(root, file), arcname=file)\n\nprint(f\"크롤링 완료, {zip_filename} 파일 저장됨.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T02:35:48.669137Z","iopub.execute_input":"2025-04-09T02:35:48.669440Z","iopub.status.idle":"2025-04-09T02:36:28.130005Z","shell.execute_reply.started":"2025-04-09T02:35:48.669410Z","shell.execute_reply":"2025-04-09T02:36:28.128786Z"}},"outputs":[{"name":"stdout","text":"크롤링 완료, naver_news.zip 파일 저장됨.\n","output_type":"stream"}],"execution_count":1}]}